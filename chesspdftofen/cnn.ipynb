{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "\n",
    "import cv2\n",
    "import os\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device cpu\n",
      "Net(\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (batch1): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (batch2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (drop1): Dropout(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=2704, out_features=120, bias=True)\n",
      "  (drop2): Dropout(p=0.4, inplace=False)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=14, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        self.batch1 = nn.BatchNorm2d(6)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.batch2 = nn.BatchNorm2d(16)\n",
    "        \n",
    "        self.drop1 = nn.Dropout(0.5)\n",
    "        self.fc1 = nn.Linear(16 * 13 * 13, 120)\n",
    "        \n",
    "        self.drop2 = nn.Dropout(0.4)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 14)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # conv\n",
    "#         x = self.pool(F.relu(self.conv1(x)))\n",
    "#         x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.pool(self.batch1(F.relu(self.conv1(x))))\n",
    "        x = self.pool(self.batch2(F.relu(self.conv2(x))))\n",
    "        \n",
    "        x = x.view(-1, 16 * 13 * 13)\n",
    "        \n",
    "        # fc\n",
    "#         x = self.drop1(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "#         x = self.drop2(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('device', device)\n",
    "net = Net()\n",
    "net.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numbers\n",
    "import numpy as np\n",
    "from PIL import ImageFilter\n",
    "\n",
    "class GaussianSmoothing(object):\n",
    "    def __init__(self, radius):\n",
    "        if isinstance(radius, numbers.Number):\n",
    "            self.min_radius = radius\n",
    "            self.max_radius = radius\n",
    "        elif isinstance(radius, list):\n",
    "            if len(radius) != 2:\n",
    "                raise Exception(\n",
    "                    \"`radius` should be a number or a list of two numbers\")\n",
    "            if radius[1] < radius[0]:\n",
    "                raise Exception(\n",
    "                    \"radius[0] should be <= radius[1]\")\n",
    "            self.min_radius = radius[0]\n",
    "            self.max_radius = radius[1]\n",
    "        else:\n",
    "            raise Exception(\n",
    "                \"`radius` should be a number or a list of two numbers\")\n",
    "\n",
    "    def __call__(self, image):\n",
    "        radius = np.random.uniform(self.min_radius, self.max_radius)\n",
    "        return image.filter(ImageFilter.GaussianBlur(radius))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77930 19483\n"
     ]
    }
   ],
   "source": [
    "from copy import copy, deepcopy\n",
    "train_transform = torchvision.transforms.Compose([\n",
    "                                            torchvision.transforms.Grayscale(),\n",
    "                                            torchvision.transforms.Resize((64, 64)),\n",
    "                                            GaussianSmoothing([0, 0.5]),\n",
    "                                            torchvision.transforms.RandomApply([\n",
    "                                              torchvision.transforms.RandomResizedCrop((64, 64), scale=(0.8, 1.0), ratio=(0.9, 1.1)),\n",
    "                                            ], p=0.75),\n",
    "                                            torchvision.transforms.ToTensor(),\n",
    "#                                             torchvision.transforms.RandomErasing(p=0.30, scale=(0.05, 0.2)),\n",
    "#                                             torchvision.transforms.Normalize((0.5, ), (0.5, ))\n",
    "])\n",
    "\n",
    "test_transform = torchvision.transforms.Compose([\n",
    "                                            torchvision.transforms.Grayscale(),\n",
    "                                            torchvision.transforms.Resize((64, 64)),\n",
    "#                                             GaussianSmoothing([0, 5]),\n",
    "                                            torchvision.transforms.ToTensor(),\n",
    "                                            # torchvision.transforms.Normalize((0.5, ), (0.5, ))\n",
    "])\n",
    "\n",
    "set1 = torchvision.datasets.ImageFolder('..\\\\data\\\\out\\\\1n_final', transform=None)\n",
    "set2 = torchvision.datasets.ImageFolder('..\\\\data\\\\out\\\\23_final', transform=None)\n",
    "set3 = torchvision.datasets.ImageFolder('..\\\\data\\\\out\\\\gm_final', transform=None)\n",
    "set4 = torchvision.datasets.ImageFolder('..\\\\data\\\\out\\\\yasser_final', transform=None)\n",
    "fullset = torch.utils.data.ConcatDataset([set1, set2, set3, set4])\n",
    "train_size = int(0.8 * len(fullset))\n",
    "# train_size = len(fullset)\n",
    "test_size = len(fullset) - train_size\n",
    "trainset, testset = torch.utils.data.random_split(fullset, [train_size, test_size])\n",
    "\n",
    "trainset.dataset = deepcopy(fullset)\n",
    "for dt in trainset.dataset.datasets: \n",
    "  dt.transform = train_transform\n",
    "\n",
    "for dt in testset.dataset.datasets: \n",
    "  dt.transform = test_transform\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset,\n",
    "                                          batch_size = 100,\n",
    "                                          shuffle = True,\n",
    "                                          num_workers = 0,\n",
    "                                          pin_memory = True)\n",
    "testloader = torch.utils.data.DataLoader(testset,\n",
    "                                         batch_size = 100,\n",
    "                                         shuffle = True,\n",
    "                                         num_workers = 0,\n",
    "                                         pin_memory = True)\n",
    "print(len(trainset), len(testset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2591575158016 Compose(\n",
      "    Grayscale(num_output_channels=1)\n",
      "    Resize(size=(64, 64), interpolation=PIL.Image.BILINEAR)\n",
      "    <__main__.GaussianSmoothing object at 0x0000025B0C922460>\n",
      "    RandomApply(\n",
      "    p=0.5\n",
      "    RandomResizedCrop(size=(64, 64), scale=(0.8, 1.0), ratio=(0.9, 1.1), interpolation=PIL.Image.BILINEAR)\n",
      ")\n",
      "    ToTensor()\n",
      "    <torchvision.transforms.transforms.RandomErasing object at 0x0000025B0C922FD0>\n",
      ")\n",
      "2590076185856 Compose(\n",
      "    Grayscale(num_output_channels=1)\n",
      "    Resize(size=(64, 64), interpolation=PIL.Image.BILINEAR)\n",
      "    <__main__.GaussianSmoothing object at 0x0000025B0C9222E0>\n",
      "    ToTensor()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(id(trainset.dataset.datasets[0]), trainset.dataset.datasets[0].transform)\n",
    "print(id(testset.dataset.datasets[0]), testset.dataset.datasets[0].transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = 'exp_cbnn1_d0.5_0.4_bn_rc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 [1,    99] loss: 1.566\n",
      "199 [1,   199] loss: 0.430\n",
      "299 [1,   299] loss: 0.186\n",
      "399 [1,   399] loss: 0.105\n",
      "499 [1,   499] loss: 0.079\n",
      "599 [1,   599] loss: 0.058\n",
      "699 [1,   699] loss: 0.048\n",
      "799 [2,    19] loss: 0.007\n",
      "899 [2,   119] loss: 0.046\n",
      "999 [2,   219] loss: 0.033\n",
      "1099 [2,   319] loss: 0.031\n",
      "1199 [2,   419] loss: 0.027\n",
      "1299 [2,   519] loss: 0.028\n",
      "1399 [2,   619] loss: 0.025\n",
      "1499 [2,   719] loss: 0.025\n",
      "1599 [3,    39] loss: 0.009\n",
      "1699 [3,   139] loss: 0.023\n",
      "1799 [3,   239] loss: 0.019\n",
      "1899 [3,   339] loss: 0.019\n",
      "1999 [3,   439] loss: 0.019\n",
      "2099 [3,   539] loss: 0.017\n",
      "2199 [3,   639] loss: 0.018\n",
      "2299 [3,   739] loss: 0.015\n",
      "2399 [4,    59] loss: 0.009\n",
      "2499 [4,   159] loss: 0.013\n",
      "2599 [4,   259] loss: 0.015\n",
      "2699 [4,   359] loss: 0.016\n",
      "2799 [4,   459] loss: 0.013\n",
      "2899 [4,   559] loss: 0.016\n",
      "2999 [4,   659] loss: 0.015\n",
      "3099 [4,   759] loss: 0.011\n",
      "3199 [5,    79] loss: 0.009\n",
      "3299 [5,   179] loss: 0.014\n",
      "3399 [5,   279] loss: 0.011\n",
      "3499 [5,   379] loss: 0.013\n",
      "3599 [5,   479] loss: 0.010\n",
      "3699 [5,   579] loss: 0.009\n",
      "3799 [5,   679] loss: 0.011\n",
      "3899 [5,   779] loss: 0.010\n",
      "3999 [6,    99] loss: 0.011\n",
      "4099 [6,   199] loss: 0.008\n",
      "4199 [6,   299] loss: 0.009\n",
      "4299 [6,   399] loss: 0.009\n",
      "4399 [6,   499] loss: 0.009\n",
      "4499 [6,   599] loss: 0.011\n",
      "4599 [6,   699] loss: 0.013\n",
      "4699 [7,    19] loss: 0.001\n",
      "4799 [7,   119] loss: 0.010\n",
      "4899 [7,   219] loss: 0.007\n",
      "4999 [7,   319] loss: 0.009\n",
      "5099 [7,   419] loss: 0.010\n",
      "5199 [7,   519] loss: 0.008\n",
      "5299 [7,   619] loss: 0.009\n",
      "5399 [7,   719] loss: 0.006\n",
      "5499 [8,    39] loss: 0.003\n",
      "5599 [8,   139] loss: 0.010\n",
      "5699 [8,   239] loss: 0.007\n",
      "5799 [8,   339] loss: 0.007\n",
      "5899 [8,   439] loss: 0.008\n",
      "5999 [8,   539] loss: 0.007\n",
      "6099 [8,   639] loss: 0.008\n",
      "6199 [8,   739] loss: 0.006\n",
      "6299 [9,    59] loss: 0.005\n",
      "6399 [9,   159] loss: 0.006\n",
      "6499 [9,   259] loss: 0.007\n",
      "6599 [9,   359] loss: 0.008\n",
      "6699 [9,   459] loss: 0.006\n",
      "6799 [9,   559] loss: 0.007\n",
      "6899 [9,   659] loss: 0.006\n",
      "6999 [9,   759] loss: 0.004\n",
      "7099 [10,    79] loss: 0.003\n",
      "7199 [10,   179] loss: 0.005\n",
      "7299 [10,   279] loss: 0.005\n",
      "7399 [10,   379] loss: 0.009\n",
      "7499 [10,   479] loss: 0.007\n",
      "7599 [10,   579] loss: 0.006\n",
      "7699 [10,   679] loss: 0.006\n",
      "7799 [10,   779] loss: 0.005\n",
      "7899 [11,    99] loss: 0.005\n",
      "7999 [11,   199] loss: 0.003\n",
      "8099 [11,   299] loss: 0.003\n",
      "8199 [11,   399] loss: 0.009\n",
      "8299 [11,   499] loss: 0.007\n",
      "8399 [11,   599] loss: 0.007\n",
      "8499 [11,   699] loss: 0.006\n",
      "8599 [12,    19] loss: 0.001\n",
      "8699 [12,   119] loss: 0.006\n",
      "8799 [12,   219] loss: 0.005\n",
      "8899 [12,   319] loss: 0.003\n",
      "8999 [12,   419] loss: 0.005\n",
      "9099 [12,   519] loss: 0.006\n",
      "9199 [12,   619] loss: 0.005\n",
      "9299 [12,   719] loss: 0.005\n",
      "9399 [13,    39] loss: 0.002\n",
      "9499 [13,   139] loss: 0.006\n",
      "9599 [13,   239] loss: 0.005\n",
      "9699 [13,   339] loss: 0.004\n",
      "9799 [13,   439] loss: 0.005\n",
      "9899 [13,   539] loss: 0.006\n",
      "9999 [13,   639] loss: 0.005\n",
      "10099 [13,   739] loss: 0.005\n",
      "10199 [14,    59] loss: 0.002\n",
      "10299 [14,   159] loss: 0.003\n",
      "10399 [14,   259] loss: 0.005\n",
      "10499 [14,   359] loss: 0.006\n",
      "10599 [14,   459] loss: 0.004\n",
      "10699 [14,   559] loss: 0.004\n",
      "10799 [14,   659] loss: 0.005\n",
      "10899 [14,   759] loss: 0.004\n",
      "10999 [15,    79] loss: 0.002\n",
      "11099 [15,   179] loss: 0.004\n",
      "11199 [15,   279] loss: 0.003\n",
      "11299 [15,   379] loss: 0.004\n",
      "11399 [15,   479] loss: 0.003\n",
      "11499 [15,   579] loss: 0.005\n",
      "11599 [15,   679] loss: 0.005\n",
      "11699 [15,   779] loss: 0.004\n",
      "11799 [16,    99] loss: 0.004\n",
      "11899 [16,   199] loss: 0.004\n",
      "11999 [16,   299] loss: 0.005\n",
      "12099 [16,   399] loss: 0.004\n",
      "12199 [16,   499] loss: 0.002\n",
      "12299 [16,   599] loss: 0.005\n",
      "12399 [16,   699] loss: 0.004\n",
      "12499 [17,    19] loss: 0.001\n",
      "12599 [17,   119] loss: 0.004\n",
      "12699 [17,   219] loss: 0.003\n",
      "12799 [17,   319] loss: 0.005\n",
      "12899 [17,   419] loss: 0.005\n",
      "12999 [17,   519] loss: 0.005\n",
      "13099 [17,   619] loss: 0.004\n",
      "13199 [17,   719] loss: 0.002\n",
      "13299 [18,    39] loss: 0.001\n",
      "13399 [18,   139] loss: 0.005\n",
      "13499 [18,   239] loss: 0.004\n",
      "13599 [18,   339] loss: 0.003\n",
      "13699 [18,   439] loss: 0.006\n",
      "13799 [18,   539] loss: 0.003\n",
      "13899 [18,   639] loss: 0.003\n",
      "13999 [18,   739] loss: 0.003\n",
      "14099 [19,    59] loss: 0.002\n",
      "14199 [19,   159] loss: 0.004\n",
      "14299 [19,   259] loss: 0.003\n",
      "14399 [19,   359] loss: 0.003\n",
      "14499 [19,   459] loss: 0.003\n",
      "14599 [19,   559] loss: 0.003\n",
      "14699 [19,   659] loss: 0.003\n",
      "14799 [19,   759] loss: 0.004\n",
      "14899 [20,    79] loss: 0.002\n",
      "14999 [20,   179] loss: 0.003\n",
      "15099 [20,   279] loss: 0.003\n",
      "15199 [20,   379] loss: 0.004\n",
      "15299 [20,   479] loss: 0.003\n",
      "15399 [20,   579] loss: 0.003\n",
      "15499 [20,   679] loss: 0.006\n",
      "15599 [20,   779] loss: 0.004\n",
      "15699 [21,    99] loss: 0.004\n",
      "15799 [21,   199] loss: 0.005\n",
      "15899 [21,   299] loss: 0.004\n",
      "15999 [21,   399] loss: 0.003\n",
      "16099 [21,   499] loss: 0.002\n",
      "16199 [21,   599] loss: 0.003\n",
      "16299 [21,   699] loss: 0.004\n",
      "16399 [22,    19] loss: 0.000\n",
      "16499 [22,   119] loss: 0.003\n",
      "16599 [22,   219] loss: 0.006\n",
      "16699 [22,   319] loss: 0.004\n",
      "16799 [22,   419] loss: 0.002\n",
      "16899 [22,   519] loss: 0.004\n",
      "16999 [22,   619] loss: 0.002\n",
      "17099 [22,   719] loss: 0.002\n",
      "17199 [23,    39] loss: 0.000\n",
      "17299 [23,   139] loss: 0.002\n",
      "17399 [23,   239] loss: 0.005\n",
      "17499 [23,   339] loss: 0.001\n",
      "17599 [23,   439] loss: 0.002\n",
      "17699 [23,   539] loss: 0.004\n",
      "17799 [23,   639] loss: 0.003\n",
      "17899 [23,   739] loss: 0.004\n",
      "17999 [24,    59] loss: 0.001\n",
      "18099 [24,   159] loss: 0.003\n",
      "18199 [24,   259] loss: 0.004\n",
      "18299 [24,   359] loss: 0.004\n",
      "18399 [24,   459] loss: 0.002\n",
      "18499 [24,   559] loss: 0.004\n",
      "18599 [24,   659] loss: 0.003\n",
      "18699 [24,   759] loss: 0.003\n",
      "18799 [25,    79] loss: 0.002\n",
      "18899 [25,   179] loss: 0.003\n",
      "18999 [25,   279] loss: 0.003\n",
      "19099 [25,   379] loss: 0.003\n",
      "19199 [25,   479] loss: 0.003\n",
      "19299 [25,   579] loss: 0.003\n",
      "19399 [25,   679] loss: 0.002\n",
      "19499 [25,   779] loss: 0.002\n",
      "19599 [26,    99] loss: 0.003\n",
      "19699 [26,   199] loss: 0.001\n",
      "19799 [26,   299] loss: 0.003\n",
      "19899 [26,   399] loss: 0.003\n",
      "19999 [26,   499] loss: 0.002\n",
      "20099 [26,   599] loss: 0.002\n",
      "20199 [26,   699] loss: 0.003\n",
      "20299 [27,    19] loss: 0.001\n",
      "20399 [27,   119] loss: 0.004\n",
      "20499 [27,   219] loss: 0.002\n",
      "20599 [27,   319] loss: 0.002\n",
      "20699 [27,   419] loss: 0.002\n",
      "20799 [27,   519] loss: 0.003\n",
      "20899 [27,   619] loss: 0.003\n",
      "20999 [27,   719] loss: 0.002\n",
      "21099 [28,    39] loss: 0.001\n",
      "21199 [28,   139] loss: 0.004\n",
      "21299 [28,   239] loss: 0.003\n",
      "21399 [28,   339] loss: 0.003\n",
      "21499 [28,   439] loss: 0.003\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "x = 0\n",
    "n = 100\n",
    "for epoch in range(30):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        x += 1\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if x % n == n-1:\n",
    "            PATH = '..\\\\data\\\\out\\\\model_' + exp + str(x) + '.pth'\n",
    "            torch.save(net.state_dict(), PATH)\n",
    "            \n",
    "            print('%d [%d, %5d] loss: %.3f' %\n",
    "                  (x, epoch + 1, i + 1, running_loss / n))\n",
    "            running_loss = 0.0\n",
    "\n",
    "PATH = '..\\\\data\\\\out\\\\model_' + exp + str(x) + '.pth'\n",
    "print(PATH)\n",
    "torch.save(net.state_dict(), PATH)\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13412\n"
     ]
    }
   ],
   "source": [
    "test_transform = torchvision.transforms.Compose([\n",
    "                                            torchvision.transforms.Grayscale(),\n",
    "                                            torchvision.transforms.Resize((64, 64)),\n",
    "                                            torchvision.transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "testset = torchvision.datasets.ImageFolder('..\\\\data\\\\out\\\\aagard_final', transform=test_transform)\n",
    "testloader = torch.utils.data.DataLoader(testset,\n",
    "                                         batch_size = 100,\n",
    "                                         shuffle = True,\n",
    "                                         num_workers = 0,\n",
    "                                         pin_memory = True)\n",
    "print(len(testset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000\n",
      "6000\n",
      "9000\n",
      "12000\n",
      "Wall time: 35.8 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv1): Conv2d(1, 16, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (conv2): Conv2d(16, 32, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (drop1): Dropout(p=0.5, inplace=False)\n",
       "  (fc1): Linear(in_features=5408, out_features=120, bias=True)\n",
       "  (drop2): Dropout(p=0.4, inplace=False)\n",
       "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
       "  (fc3): Linear(in_features=84, out_features=14, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# wtf\n",
    "correct = 0\n",
    "total = 0\n",
    "# 23\n",
    "# num_images = 28032 \n",
    "# 1n\n",
    "# num_images = 26816\n",
    "\n",
    "cm = np.zeros((14, 14), dtype=np.int64)\n",
    "net.eval()\n",
    "with torch.no_grad():\n",
    "  for data in testloader:\n",
    "    images, labels = data[0].to(device), data[1].to(device)\n",
    "    outputs = net(images)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    cm += confusion_matrix(labels.numpy(), predicted.numpy(), labels=np.arange(14))\n",
    "    total += labels.size(0)\n",
    "    if total % 3000 == 0:\n",
    "      print(total)\n",
    "#     if total > 3000:\n",
    "#       break\n",
    "#     correct = (predicted == labels).sum().item()\n",
    "#     if correct != predicted.shape[0]:\n",
    "#       break\n",
    "\n",
    "# print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
    "#     100 * correct / total))\n",
    "net.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0    1    2    3    4    5    6    7    8    9   10   11   12   13\n",
      "[[1032    0    0   20    0    1   11   34    0    0   29    1    0    4]\n",
      " [ 128  491    0   82    2    0    9    3    0    0    0    0    0    0]\n",
      " [   2    0  857    5    0    0    0    2    0    0   51    0    0    4]\n",
      " [   0    0    0 1007    0    0   58    2    0    1    6    1    2  113]\n",
      " [   0    1    0    1  690    0    0    0    0    0    0   34    0    0]\n",
      " [   0    1    0   17    1  957    1    5    0    0    8    0   23    1]\n",
      " [   0    0    0    0    0    0 1083    0    0    0    0    0    0    0]\n",
      " [   1    0    0    1    0    0    5  963    0    0   38    0    3    1]\n",
      " [   0    0    0    5    0    1    1   94  444    0    4    0  264    1]\n",
      " [   0    0    0    0    0    0    0   18    0  853   31    0    0    0]\n",
      " [   0    0    0    0    0    0    8    0    0    2 1014    0    0    8]\n",
      " [   2    0    0    2   16    0    1    0    0    0    1  743    0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    2    0 1011    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0 1093]]\n",
      "accuracy 0.9124664479570533\n",
      "total 12238 13412\n"
     ]
    }
   ],
   "source": [
    "total = np.sum(cm)\n",
    "print('     0    1    2    3    4    5    6    7    8    9   10   11   12   13')\n",
    "print(cm.astype(np.int32))\n",
    "print('accuracy', np.trace(cm) / total)\n",
    "print('total', np.trace(cm), total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.91166078, 0.68671329, 0.93051031, 0.84621849, 0.95041322,\n",
       "       0.94378698, 1.        , 0.95158103, 0.54545455, 0.94567627,\n",
       "       0.98255814, 0.97124183, 0.99802567, 1.        ])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm.diagonal() / cm.sum(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "def sw(im):\n",
    "  plt.imshow(cv2.cvtColor(im, cv2.COLOR_BGR2RGB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPUAAAD4CAYAAAA0L6C7AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy86wFpkAAAACXBIWXMAAAsTAAALEwEAmpwYAAAJk0lEQVR4nO3dz2ucBR7H8c/HNJrWLgquh9LUjYeiW4VVCKXQW/UQf6BXBT0JvaxQQRA9+g+IFy9BxQWlIuhBi4sEVERw1aq1mK1CEBeLQreI1BZpG/vZQ8LSdZPOM9N55sl8eb8gkMkMMx9K33kyk/CMkwhAHVd0PQDAcBE1UAxRA8UQNVAMUQPFbGrjTm2PzUvqU1NTXU/oyxVXjNf34c2bN3c9obELFy50PaGxM2fO6OzZs17rulaiHiczMzNdT+jL1q1bu57Ql1tuuaXrCY39+uuvXU9obGFhYd3rxuvbPoCeiBoohqiBYogaKIaogWKIGiiGqIFiiBoohqiBYogaKIaogWKIGiiGqIFiiBoohqiBYogaKIaogWIaRW17zvY3tpdsP9n2KACD6xm17QlJz0m6S9IuSQ/a3tX2MACDaXKk3i1pKcm3Sc5JelXS/e3OAjCoJlFvl/T9RZePr37tf9jeb/uw7cPDGgegf03OJrrWaUj/7xTASeYlzUvjdYpgoJomR+rjknZcdHla0g/tzAFwuZpE/amknbZvtH2lpAckvdnuLACD6vnjd5Jl249KekfShKQXkyy2vgzAQBq9Q0eStyW93fIWAEPAX5QBxRA1UAxRA8UQNVAMUQPFEDVQDFEDxRA1UAxRA8UQNVAMUQPFEDVQDFEDxRA1UAxRA8UQNVBMo5Mk9Ouqq67S9PR0G3c9dHv27Ol6Ql/eeuutrif05dSpU11PaOyGG27oekJjy8vL617HkRoohqiBYogaKIaogWKIGiiGqIFiiBoohqiBYogaKIaogWKIGiiGqIFiiBoohqiBYogaKIaogWKIGiimZ9S2X7R9wvZXoxgE4PI0OVK/JGmu5R0AhqRn1Ek+kPTTCLYAGAKeUwPFDO1sorb3S9ovSZs2tXKSUgANDO1InWQ+yWyS2YmJiWHdLYA+8eM3UEyTX2kdlPSRpJtsH7f9SPuzAAyq55PfJA+OYgiA4eDHb6AYogaKIWqgGKIGiiFqoBiiBoohaqAYogaKIWqgGKIGiiFqoBiiBoohaqAYogaKIWqgGKIGinGSod/pddddl7m58ThV+NLSUtcT+nL+/PmuJ/Tl5ptv7npCYwcPHux6Ql+SeK2vc6QGiiFqoBiiBoohaqAYogaKIWqgGKIGiiFqoBiiBoohaqAYogaKIWqgGKIGiiFqoBiiBoohaqAYogaKIWqgmJ5R295h+z3bx2wv2j4wimEABrOpwW2WJT2e5HPbf5D0me2FJP9seRuAAfQ8Uif5Mcnnq5//IumYpO1tDwMwmCZH6v+yPSPpdkkfr3Hdfkn7JWnLli3D2AZgAI1fKLO9VdLrkh5Lcur31yeZTzKbZHZqamqYGwH0oVHUtie1EvQrSd5odxKAy9Hk1W9LekHSsSTPtD8JwOVocqTeK+lhSftsH1n9uLvlXQAG1POFsiQfSlrz7T0AbDz8RRlQDFEDxRA1UAxRA8UQNVAMUQPFEDVQDFEDxRA1UAxRA8UQNVAMUQPFEDVQDFEDxRA1UAxRA8U4ydDvdMuWLdm5c+fQ77cNV199ddcT+nLmzJmuJ/Tl6NGjXU9obNu2bV1PaOzkyZM6d+7cmicv4UgNFEPUQDFEDRRD1EAxRA0UQ9RAMUQNFEPUQDFEDRRD1EAxRA0UQ9RAMUQNFEPUQDFEDRRD1EAxRA0U0zNq21O2P7H9pe1F20+PYhiAwWxqcJuzkvYlOW17UtKHtv+e5B8tbwMwgJ5RZ+UkZqdXL06ufgz/xGYAhqLRc2rbE7aPSDohaSHJx62uAjCwRlEn+S3JbZKmJe22fevvb2N7v+3Dtg8vLy8PeSaApvp69TvJz5LelzS3xnXzSWaTzG7a1OSpOoA2NHn1+3rb165+vlnSnZK+bnkXgAE1OaRuk/Q32xNa+SbwWpJD7c4CMKgmr34flXT7CLYAGAL+ogwohqiBYogaKIaogWKIGiiGqIFiiBoohqiBYogaKIaogWKIGiiGqIFiiBoohqiBYogaKIaogWJaOZnYxMSErrnmmjbueuhOnz7d+0YbyOLiYtcT+nLHHXd0PaGx6enpric0dujQ+icf4kgNFEPUQDFEDRRD1EAxRA0UQ9RAMUQNFEPUQDFEDRRD1EAxRA0UQ9RAMUQNFEPUQDFEDRRD1EAxRA0UQ9RAMY2jtj1h+wvb659HBUDn+jlSH5B0rK0hAIajUdS2pyXdI+n5ducAuFxNj9TPSnpC0oX1bmB7v+3Dtg+fP39+GNsADKBn1LbvlXQiyWeXul2S+SSzSWYnJyeHNhBAf5ocqfdKus/2d5JelbTP9sutrgIwsJ5RJ3kqyXSSGUkPSHo3yUOtLwMwEH5PDRTT19vuJHlf0vutLAEwFBypgWKIGiiGqIFiiBoohqiBYogaKIaogWKIGiiGqIFiiBoohqiBYogaKIaogWKIGiiGqIFiiBooxkmGf6f2vyX9a8h3+0dJJ4d8n20ap73jtFUar71tbf1TkuvXuqKVqNtg+3CS2a53NDVOe8dpqzRee7vYyo/fQDFEDRQzTlHPdz2gT+O0d5y2SuO1d+Rbx+Y5NYBmxulIDaABogaKGYuobc/Z/sb2ku0nu95zKbZftH3C9lddb+nF9g7b79k+ZnvR9oGuN63H9pTtT2x/ubr16a43NWF7wvYXtg+N6jE3fNS2JyQ9J+kuSbskPWh7V7erLuklSXNdj2hoWdLjSf4saY+kv27gf9uzkvYl+Yuk2yTN2d7T7aRGDkg6NsoH3PBRS9otaSnJt0nOaeWdN+/veNO6knwg6aeudzSR5Mckn69+/otW/vNt73bV2rLi9OrFydWPDf0qr+1pSfdIen6UjzsOUW+X9P1Fl49rg/7HG2e2ZyTdLunjjqesa/VH2SOSTkhaSLJht656VtITki6M8kHHIWqv8bUN/R163NjeKul1SY8lOdX1nvUk+S3JbZKmJe22fWvHk9Zl+15JJ5J8NurHHoeoj0vacdHlaUk/dLSlHNuTWgn6lSRvdL2niSQ/a+XdVzfyaxd7Jd1n+zutPGXcZ/vlUTzwOET9qaSdtm+0faVW3vj+zY43lWDbkl6QdCzJM13vuRTb19u+dvXzzZLulPR1p6MuIclTSaaTzGjl/+y7SR4axWNv+KiTLEt6VNI7Wnkh57Uki92uWp/tg5I+knST7eO2H+l60yXslfSwVo4iR1Y/7u561Dq2SXrP9lGtfKNfSDKyXxONE/5MFChmwx+pAfSHqIFiiBoohqiBYogaKIaogWKIGijmP+Kr/YGyF/uhAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sw(net.conv1.weight[5,...].detach().numpy().squeeze(0))\n",
    "# net.conv2.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device cpu\n",
      "Net(\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv1): Conv2d(1, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(16, 32, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (drop1): Dropout(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=5408, out_features=120, bias=True)\n",
      "  (drop2): Dropout(p=0.4, inplace=False)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=14, bias=True)\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv1): Conv2d(1, 16, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (conv2): Conv2d(16, 32, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (drop1): Dropout(p=0.5, inplace=False)\n",
       "  (fc1): Linear(in_features=5408, out_features=120, bias=True)\n",
       "  (drop2): Dropout(p=0.4, inplace=False)\n",
       "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
       "  (fc3): Linear(in_features=84, out_features=14, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lol\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(1, 16, 5)\n",
    "#         self.batch1 = nn.BatchNorm2d(6)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(16, 32, 5)\n",
    "#         self.batch2 = nn.BatchNorm2d(16)\n",
    "        \n",
    "        self.drop1 = nn.Dropout(0.5)\n",
    "        self.fc1 = nn.Linear(32 * 13 * 13, 120)\n",
    "        \n",
    "        self.drop2 = nn.Dropout(0.4)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 14)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # conv\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "#         x = self.pool(self.batch1(F.relu(self.conv1(x))))\n",
    "#         x = self.pool(self.batch2(F.relu(self.conv2(x))))\n",
    "        \n",
    "        x = x.view(-1, 32 * 13 * 13)\n",
    "        \n",
    "        # fc\n",
    "        x = self.drop1(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.drop2(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('device', device)\n",
    "net = Net()\n",
    "net.to(device)\n",
    "# PATH = '..//data//out//model_exp_model9_4390.pth'\n",
    "# PATH = '..//data//out//model_exp_model12_6770.pth'\n",
    "# PATH = '..//data//out//model_exp_cbnn1_d0.4_0.3_7800.pth'\n",
    "PATH = '..//data//out//model_exp_cbnn3_d0.5_0.4_rc_re_7800.pth'\n",
    "net.load_state_dict(torch.load(PATH))\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "print(net)\n",
    "net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 5,  2,  0,  4,  1,  0,  2,  5,  3,  3,  3,  3,  3,  3,  3,  3, 13,  6,\n",
      "        13,  6, 13,  6, 13,  6,  6, 13,  6, 13,  6, 13,  6, 13, 13,  6, 13,  6,\n",
      "        13,  6, 13,  6,  6, 13,  6, 13,  6, 13,  6, 13, 10, 10, 10, 10, 10, 10,\n",
      "        10, 10, 12,  9,  7, 11,  8,  7,  9, 12])\n",
      "rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR%20w%20KQkq%20-%200%201\n"
     ]
    }
   ],
   "source": [
    "image_path = '..\\\\data\\\\out\\\\a22.png'\n",
    "board_im = cv2.imread(image_path, 0)\n",
    "\n",
    "transform = torchvision.transforms.Compose([\n",
    "  # torchvision.transforms.Grayscale(),\n",
    "  torchvision.transforms.Resize((8*64, 8*64)),\n",
    "#   GaussianSmoothing([0, 0.5]),\n",
    "  torchvision.transforms.ToTensor(),\n",
    "  # torchvision.transforms.Normalize((0.5, ), (0.5, ))\n",
    "])\n",
    "\n",
    "im = Image.fromarray(board_im)\n",
    "im = transform(im)\n",
    "dim = 64\n",
    "tensors = [im[:, dim*k: dim*(k+1), dim*j: dim*(j+1)] for k in range(8) for j in range(8)]\n",
    "# sw(tensors[0].numpy()[np.newaxis, ...] * 255)\n",
    "# print(tensors[0].numpy()[np.newaxis, ...])\n",
    "images = torch.stack(tensors)\n",
    "outputs = net(images)\n",
    "_, predicted = torch.max(outputs.data, 1)\n",
    "print(predicted)\n",
    "print(get_fen_str(predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 21.2408,   4.6339,   7.8342,  27.8178,   2.3713,  23.7090,  -7.5043,\n",
       "           9.4107,  -3.8514, -15.6906,  -8.4994,  -1.2262, -13.4902, -27.4314]])"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net(images[0,...].unsqueeze(0)).data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sw((images[0] * 255).view(64, 64, 1).numpy().astype(np.uint8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "piecenames = ['bb', 'bk', 'bn', 'bp', 'bq', 'br', 'em', 'wb', 'wk', 'wn', 'wp', 'wq', 'wr', 'em']\n",
    "\n",
    "def get_fen_str(predicted):\n",
    "  with io.StringIO() as s:\n",
    "    for row in range(8):\n",
    "      empty = 0\n",
    "      for cell in range(8):\n",
    "        c = piecenames[predicted[row*8 + cell]]\n",
    "        if c[0] in ('w', 'b'):\n",
    "          if empty > 0:\n",
    "            s.write(str(empty))\n",
    "            empty = 0\n",
    "          s.write(c[1].upper() if c[0] == 'w' else c[1].lower())\n",
    "        else:\n",
    "          empty += 1\n",
    "\n",
    "      if empty > 0:\n",
    "        s.write(str(empty))\n",
    "      s.write('/')\n",
    "    # Move one position back to overwrite last '/'\n",
    "    s.seek(s.tell() - 1)\n",
    "    # If you do not have the additional information choose what to put\n",
    "    # s.write(' w KQkq - 0 1')\n",
    "    s.write('%20w%20KQkq%20-%200%201')\n",
    "    return s.getvalue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.0118, 0.0000, 0.0078,  ..., 0.0078, 0.0471, 0.0196],\n",
      "         [0.0000, 0.0078, 0.0235,  ..., 0.2314, 0.2471, 0.1490],\n",
      "         [0.1608, 0.1569, 0.1686,  ..., 0.9961, 0.9725, 0.7294],\n",
      "         ...,\n",
      "         [0.9725, 1.0000, 0.9922,  ..., 0.9922, 0.9804, 0.9961],\n",
      "         [0.9882, 0.9765, 0.9922,  ..., 0.6196, 0.6118, 0.6118],\n",
      "         [0.1804, 0.1961, 0.1373,  ..., 0.0784, 0.0745, 0.0706]]])\n"
     ]
    }
   ],
   "source": [
    "def pil_loader(path):\n",
    "  # open path as file to avoid ResourceWarning (https://github.com/python-pillow/Pillow/issues/835)\n",
    "  with open(path, 'rb') as f:\n",
    "    img = Image.open(f)\n",
    "    return img.convert('L')\n",
    "# print(np.array(pil_loader(image_path)))\n",
    "print(transform(pil_loader(image_path)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "%matplotlib inline\n",
    "def sbw(im):\n",
    "  plt.imshow(im, cmap='gray', vmin=0, vmax=255)\n",
    "\n",
    "def sw(im):\n",
    "  plt.imshow(cv2.cvtColor(im, cv2.COLOR_BGR2RGB))\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "piecenames = ['BlackBishop', 'BlackKing', 'BlackKnight', 'BlackPawn', 'BlackQueen', 'BlackRook', 'BlackSpace', 'WhiteBishop', 'WhiteKing', 'WhiteKnight', 'WhitePawn', 'WhiteQueen', 'WhiteRook', 'WhiteSpace']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "\n",
    "folder = '..\\\\data\\\\out\\\\yasser_final'\n",
    "x = 0\n",
    "cm = np.zeros((14, 14), dtype=np.int64)\n",
    "\n",
    "def pil_loader(path):\n",
    "  # open path as file to avoid ResourceWarning (https://github.com/python-pillow/Pillow/issues/835)\n",
    "  with open(path, 'rb') as f:\n",
    "    img = Image.open(f)\n",
    "    return img.convert('RGB')\n",
    "      \n",
    "for i, piece in enumerate(sorted(os.listdir(folder))):\n",
    "  folder2 = os.path.join(folder, piece)\n",
    "  if os.path.isfile(folder2):\n",
    "    continue\n",
    "  for j, filename in enumerate(sorted(os.listdir(folder2))):\n",
    "    fullname = os.path.join(folder2, filename)\n",
    "    im = pil_loader(fullname)\n",
    "    im = torchvision.transforms.functional.to_grayscale(im)\n",
    "    im = torchvision.transforms.functional.resize(im, (64, 64), Image.BILINEAR)\n",
    "    im = torchvision.transforms.functional.to_tensor(im)\n",
    "    im = im.unsqueeze(0)\n",
    "\n",
    "# #     im = im.unsqueeze(0).type(torch.float32)\n",
    "# #     im = cv2.imread(fullname, 0)\n",
    "# #     im = cv2.resize(im, (64, 64)).astype(np.float32)\n",
    "# #     im = torch.from_numpy(im).unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "    outputs = net(im)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "#     piece = piecenames[predicted]\n",
    "#     newpath = os.path.join(folder2, piece + '_con1_' + str(x) + '.png')\n",
    "#     print(newpath)\n",
    "#     os.rename(fullname, newpath)\n",
    "    cm[i][predicted] += 1\n",
    "    x += 1\n",
    "    if x % 1000 == 0:\n",
    "      print(x)\n",
    "#     print(predicted)\n",
    "#     break\n",
    "#   break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "resnet50 = models.resnet50(pretrained=True, progress=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vgg16' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-72-cb71977da5f6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[0mactivation\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mvgg16\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mregister_forward_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mget_activation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'fc2'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'vgg16' is not defined"
     ]
    }
   ],
   "source": [
    "activation = {}\n",
    "def get_activation(name):\n",
    "    def hook(model, input, output):\n",
    "        activation[name] = output.detach()\n",
    "    return hook\n",
    "vgg16.classifier[-2].register_forward_hook(get_activation('fc2'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.hooks.RemovableHandle at 0x2cf371c1e20>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activation = {}\n",
    "def get_activation(name):\n",
    "    def hook(model, input, output):\n",
    "        activation[name] = output.detach()\n",
    "    return hook\n",
    "resnet50.avgpool.register_forward_hook(get_activation('fc2'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\chesspdftofen\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    721\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 722\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\chesspdftofen\\lib\\site-packages\\torchvision\\models\\resnet.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    218\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    219\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 220\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    221\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    222\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\chesspdftofen\\lib\\site-packages\\torchvision\\models\\resnet.py\u001b[0m in \u001b[0;36m_forward_impl\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    206\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmaxpool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 208\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayer1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    209\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayer2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayer3\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\chesspdftofen\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    721\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 722\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\chesspdftofen\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    115\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 117\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    118\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\chesspdftofen\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    721\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 722\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\chesspdftofen\\lib\\site-packages\\torchvision\\models\\resnet.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    106\u001b[0m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbn2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\chesspdftofen\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    721\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 722\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\chesspdftofen\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    417\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    418\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 419\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    420\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    421\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\chesspdftofen\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[1;34m(self, input, weight)\u001b[0m\n\u001b[0;32m    413\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    414\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m--> 415\u001b[1;33m         return F.conv2d(input, weight, self.bias, self.stride,\n\u001b[0m\u001b[0;32m    416\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[0;32m    417\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# res = np.empty((26816, 2048), dtype=np.float32)\n",
    "# res = np.empty((28, 2048), dtype=np.float32)\n",
    "x = 0\n",
    "with torch.no_grad():\n",
    "  for data in trainloader:\n",
    "    images, labels = data[0].to(device), data[1].to(device)\n",
    "    n = images.shape[0]\n",
    "    \n",
    "    output = resnet50(images)\n",
    "#     res[x: x + n,:] = activation['fc2'].view(-1, 2048)\n",
    "    act = activation['fc2'].view(-1, 2048)\n",
    "    break\n",
    "\n",
    "    x += n\n",
    "    if x % 1000 == 0:\n",
    "      print('x', x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt = res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.07750261 0.8814883  1.3272038  ... 0.94399077 0.12172581 0.5190152 ]\n",
      " [0.53081465 1.1889176  0.87304455 ... 0.57789785 0.25939602 0.1244965 ]\n",
      " [0.15970045 0.5196016  0.4135991  ... 0.508797   1.1560974  0.50378776]\n",
      " ...\n",
      " [0.4481257  0.25660902 0.21730614 ... 0.10816965 0.05452814 0.22506453]\n",
      " [0.22708838 0.12393332 0.39125115 ... 0.03920951 0.42045596 0.23482805]\n",
      " [0.22708838 0.12393332 0.39125115 ... 0.03920951 0.42045596 0.23482805]]\n",
      "tensor([0.2012, 0.2250, 0.1623,  ..., 0.2641, 0.2916, 0.1376])\n"
     ]
    }
   ],
   "source": [
    "# act[0].shape, gt.shape\n",
    "# print(sklearn.metrics.pairwise.euclidean_distances(act, gt).argmin(axis=1) / 2)\n",
    "# print(labels)\n",
    "print(gt)\n",
    "print(act[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'bestLabels' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'bestLabels' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "cm = np.zeros((14, 14))\n",
    "x = 0\n",
    "with torch.no_grad():\n",
    "  for data in trainloader:\n",
    "    images, labels = data[0].to(device), data[1].to(device)\n",
    "    n = images.shape[0]\n",
    "    cm += confusion_matrix(labels, bestLabels[x: x+n], labels=np.arange(14))\n",
    "    x += n\n",
    "    if x % 1000 == 0:\n",
    "      print('x', x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13]),\n",
       " array([11,  3,  7,  8, 11, 13,  1, 13,  3,  3,  5,  6, 11,  6,  9],\n",
       "       dtype=int32))"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels, bestLabels[-15:].flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 2048])"
      ]
     },
     "execution_count": 502,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activation['fc2'].view(-1, 2048).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(res, open('..\\\\data\\\\out\\\\res2.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# res = np.load('..\\\\data\\\\out\\\\res.pkl')\n",
    "import pickle\n",
    "res = pickle.load(open('..\\\\data\\\\out\\\\res2.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 11.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, int(1e6), 1.0)\n",
    "ret, bestLabels, centers = cv2.kmeans(res, 14, None, criteria, 10, cv2.KMEANS_PP_CENTERS)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (chesspdftofen)",
   "language": "python",
   "name": "chesspdftofen"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
